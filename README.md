# Sensitivity Analysis - Postprocessing  

Scripts to perform the postprocessing of sensitivity analysis results obtained with **DAKOTA** and **MAMMA**.  
This repository provides tools to:  
- Extract and organize raw simulation outputs into `csv` files.  
- Generate correlation plots between input parameters and response functions.  
- Generate plot Sobol indices for variance-based sensitivity analysis.  
- (Planned) Extend functionality to frequency plots and other postprocessing tasks.  

## ðŸ“‘ Table of Contents  

1. [Repository Structure](#1-repository-structure)  
2. [Preparation](#2-preparation)  
3. [Data Extraction (`extact_allData.py`)](#3-data-extraction-extract_alldatapy)  
    * [Usage](#usage)
    * [Output](#output)
    * [Customization](#customization)
4. [Correlation Plots (`plot_correlation.py`)](#4-correlation-plots-plot_correlationpy) 
    * [Features](#features)
    * [Required files](#required-files)
    * [Usage](#usage-1)
    * [Output](#output-1)
    * [Customization](#customization-1)
5. [Sobol Indices (`plot_sobol.py`)](#5-sobol-indices-plot_sobolpy)  
    * [Output](#output-2)
6. [Notes](#notes)  

## 1. Repository Structure

- **`extract_allData.py`** â†’ Extracts all data from DAKOTA/MAMMA simulations into structured CSV files.  
- **`plot_correlation.py`** â†’ Generates correlation plots between input variables and response functions.  
- **`plot_sobol.py`** â†’ Plots Sobol sensitivity indices.  
- **`my_lib_extract_data.py`** â†’ Library of functions to support `estract_allData.py'.
- **`my_lib_process_utils.py`** â†’ Library of functions that support all the other scripts.

## 2. Preparation 

Make sure the working directory contains the following files:  

- `dakota_tabular.dat`  
- `dakota_test_parallel.in`  
- `conduit_solver.template`  

Additionally, you need a folder containing all the simulation results from the sensitivity analysis.  
A recommended structure is:  

workdir/

â”œâ”€â”€ workdir.1/

â”œâ”€â”€ workdir.2/

â”œâ”€â”€ ...


## 3. Data Extraction (`extract_allData.py`)

### Usage 
Run the extraction script to collect all simulation results into CSV files:  

```bash
python extract_allData.py

```

Optional arguments:

--verbose true/false â†’ Enable detailed logging.

--pause true/false â†’ Pause execution at key steps.

### Output

The script generates 34 CSV files organized by eruptive style (Explosive, Effusive, Fountaining, etc.) and saves them in the `csv_files/` directory.

Examples:

`data_allConcat.csv`

`data_allConcat_explosive.csv`

`data_allConcat_notExplosive.csv`

`data_allConcat_notExplosive_effusive.csv`

`data_allConcat_notExplosive_fountaining.csv`

â€¦ and others.

Each CSV file uses a *two-row header*:
- **level 0:** technical names (`x1`, `response_fn_12`, â€¦)  
- **level 1:** descriptive labels (`Pressure [Pa]`, `Exit velocity [m/s]`, â€¦)

### Customization 

The user can modify the extraction of data, by adding, removing, or modifying response functions. 
* Go to `extract_allData.py` and modify the part where new response functions are created and where the labels are generated.
  Mind that the current setup accounts for 15 response functions generated by DAKOTA outputs and are read from `dakota_tabular.dat`.
* Go to `my_lib_extract_data.py` and modify inside the functions `extract_data_*`:
  * the part related to `response_data['response_fn_*'][index_simul]`
  * the labels `'response_fn_*': "XXX [YYY]"`,


##  4. Correlation Plots (`plot_correlation.py`)

Generate correlation plots between input parameters (x1, x2, â€¦) and response functions (response_fn_*).


### Features
- **File management**  
  - Checks for required input files (see below) in the `csv_files/` folder and in the current working directory. In the latter case, moves them automatically into the `csv_files/` folder.  
  - If missing, runs `extract_allData.py` to generate them.  
- **Data handling**  
  - Reads CSVs with two-row headers into `pandas.DataFrame` objects.  
  - Computes binned statistics (`bin_and_average`) for response functions.  
- **Plotting utilities** (from `my_lib_process_utils.py`):  
  - `plot_xi_vs_response_fn`: plot automatically each `xi` present vs. one response function.  
  - `plot_lists`: plot arbitrary pairs of variables.   
  Each plotting utility supports multiple datasets overlayed in the same figure (e.g., Explosive, Effusive, and Fountaining) or a single dataset only.
  In case that multiple datasets are plotted, each one has a distinct color and - if desidered - a distinct marker style.
  Each plotting utility supports the plot of the mean values *IF* a dictionary `stats` is passed.

### Required files
- **Mandatory**  
  - `dakota_test_parallel.in` (DAKOTA input file containing parameter bounds).  
- **Optional: CSV datasets**  
  - `data_allConcat.csv`  
  - `data_allConcat_explosive.csv`  
  - `data_allConcat_notExplosive.csv`  
  - `data_allConcat_notExplosive_effusive.csv`  
  - `data_allConcat_notExplosive_fountaining.csv`  
  If present, they are loaded as dataFrame, otherwise they are generated by calling `extract_allData.py` and then loaded.

### Usage 
Run from the terminal:

```bash
python plot_correlation.py
```

### Output
* Plots show scatter distributions and binned means for comparison.
* Multiple eruptive regimes (Explosive, Effusive, Fountaining) can be compared on the same plot.
* Saves figures in `.svg` format under `plot_correlations/`.

### Customization

The script can be easily customized to:

Select specific response_fn_* to analyze.

Apply transformations (e.g., log10, scaling) to axes.

Change labels and plot layouts.



## 5. Sobol Indices (`plot_sobol.py`) 

Plot Sobol sensitivity indices for selected response functions.

```bash
python plot_Sobol.py
```

### Output

Figures saved in `plot_Sobol/`.

User can modify the script to choose which response_fn_* to analyze.


## Notes 

All scripts are designed to be modified by the user to adapt to specific workflows. For example:
- Change the label names going into...

Figures are saved in .svg format for high-quality vector graphics. The user can change it going to...

